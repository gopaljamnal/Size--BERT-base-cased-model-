# -*- coding: utf-8 -*-
"""Text_classification_with_BERT_Challenge_1_NLP_model_size.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/LinkedInLearning/transformers-text-classification-for-nlp-using-bert-2478096/blob/main/Text_classification_with_BERT_Challenge_1_NLP_model_size.ipynb

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/13To5sxVt75i4HBEI6NU_UpeLXo2fmE-S?usp=sharing)

# Challenge: NLP model size
1. How many parameters does the BERT base cased model have (bert-base-cased)?  Use the *get_model_size function* below to help you.
2. If you know the number of parameters for a model, how might you be able to  determine how much memory is required when running a model inference? (Each parameter is represented as a single precision floating point number)
3. If you wanted to run a GPT-3 inference. How much RAM would your infrastructure require.

This should take you between 5-10 minutes.
"""

!pip install transformers[sentencepiece]

from transformers import AutoModel
import torch

def get_model_size(checkpoint='bert-base-cased'):
  '''
  Usage: 
      checkpoint - this is NLP model with its configuration and its associated weights
      returns the size of the NLP model you want to determine
  '''
  
  model = AutoModel.from_pretrained(checkpoint)
  return sum(torch.numel(param) for param in model.parameters())

checkpoint='bert-base-cased'
print(f"The number of parameters for {checkpoint} is : {get_model_size(checkpoint)}")

